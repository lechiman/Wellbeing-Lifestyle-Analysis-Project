{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wellbeing and Lifestyle Data Analysis\n",
        "\n",
        "## Section 1: Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook performs comprehensive exploratory data analysis on the wellbeing and lifestyle dataset, including:\n",
        "- Data quality checks\n",
        "- Correlation analysis\n",
        "- Distribution visualizations\n",
        "- Outlier detection and handling\n",
        "- Summary statistics by demographic groups\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Load Dataset and Check Data Quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Wellbeing_and_lifestyle_data_Kaggle.csv')\n",
        "\n",
        "# Convert numeric columns to proper dtypes\n",
        "# All columns except Timestamp, AGE, and GENDER should be numeric\n",
        "numeric_columns = [col for col in df.columns if col not in ['Timestamp', 'AGE', 'GENDER']]\n",
        "for col in numeric_columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Display basic information\n",
        "print(\"=\" * 80)\n",
        "print(\"DATASET SHAPE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"=\" * 80)\n",
        "print(\"FIRST 5 ROWS\")\n",
        "print(\"=\" * 80)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data types and basic information\n",
        "print(\"=\" * 80)\n",
        "print(\"DATASET INFO\")\n",
        "print(\"=\" * 80)\n",
        "df.info()\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COLUMN NAMES\")\n",
        "print(\"=\" * 80)\n",
        "print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=\" * 80)\n",
        "print(\"MISSING VALUES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing_Count': missing_values.values,\n",
        "    'Missing_Percent': missing_percent.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(missing_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"No missing values found in the dataset!\")\n",
        "    \n",
        "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "print(\"=\" * 80)\n",
        "print(\"DUPLICATE ROWS\")\n",
        "print(\"=\" * 80)\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic descriptive statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"DESCRIPTIVE STATISTICS FOR NUMERICAL FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check categorical variables\n",
        "print(\"=\" * 80)\n",
        "print(\"CATEGORICAL VARIABLES DISTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Categorical columns: {categorical_cols}\\n\")\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col != 'Timestamp':  # Skip timestamp\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(df[col].value_counts())\n",
        "        print(f\"Unique values: {df[col].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select only numerical columns for correlation analysis\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "print(f\"Numerical columns for correlation: {len(numerical_cols)}\")\n",
        "print(numerical_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "\n",
        "# Create a large figure for the correlation heatmap\n",
        "plt.figure(figsize=(20, 16))\n",
        "sns.heatmap(correlation_matrix, \n",
        "            annot=True, \n",
        "            fmt='.2f', \n",
        "            cmap='coolwarm', \n",
        "            center=0,\n",
        "            square=True,\n",
        "            linewidths=0.5,\n",
        "            cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Heatmap of All Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation Matrix Shape:\", correlation_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find strongest correlations with WORK_LIFE_BALANCE_SCORE (target variable)\n",
        "print(\"=\" * 80)\n",
        "print(\"TOP CORRELATIONS WITH WORK_LIFE_BALANCE_SCORE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "target_correlations = correlation_matrix['WORK_LIFE_BALANCE_SCORE'].sort_values(ascending=False)\n",
        "print(\"\\nTop positive correlations:\")\n",
        "print(target_correlations.head(10))\n",
        "\n",
        "print(\"\\nTop negative correlations:\")\n",
        "print(target_correlations.tail(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top correlations with target variable\n",
        "top_features = target_correlations.abs().sort_values(ascending=False)[1:16]  # Exclude itself, take top 15\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features.sort_values().plot(kind='barh', color=['red' if x < 0 else 'green' for x in top_features.sort_values()])\n",
        "plt.title('Top 15 Features Correlated with Work-Life Balance Score', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
        "plt.ylabel('Features', fontsize=12)\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Distribution Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of the target variable (WORK_LIFE_BALANCE_SCORE)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(df['WORK_LIFE_BALANCE_SCORE'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Distribution of Work-Life Balance Score', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Work-Life Balance Score', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].axvline(df['WORK_LIFE_BALANCE_SCORE'].mean(), color='red', linestyle='--', \n",
        "                linewidth=2, label=f'Mean: {df[\"WORK_LIFE_BALANCE_SCORE\"].mean():.2f}')\n",
        "axes[0].axvline(df['WORK_LIFE_BALANCE_SCORE'].median(), color='green', linestyle='--', \n",
        "                linewidth=2, label=f'Median: {df[\"WORK_LIFE_BALANCE_SCORE\"].median():.2f}')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "axes[1].boxplot(df['WORK_LIFE_BALANCE_SCORE'], vert=True, patch_artist=True,\n",
        "                boxprops=dict(facecolor='lightblue', color='blue'),\n",
        "                medianprops=dict(color='red', linewidth=2),\n",
        "                whiskerprops=dict(color='blue'),\n",
        "                capprops=dict(color='blue'))\n",
        "axes[1].set_title('Box Plot of Work-Life Balance Score', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Work-Life Balance Score', fontsize=12)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(f\"Mean: {df['WORK_LIFE_BALANCE_SCORE'].mean():.2f}\")\n",
        "print(f\"Median: {df['WORK_LIFE_BALANCE_SCORE'].median():.2f}\")\n",
        "print(f\"Std Dev: {df['WORK_LIFE_BALANCE_SCORE'].std():.2f}\")\n",
        "print(f\"Min: {df['WORK_LIFE_BALANCE_SCORE'].min():.2f}\")\n",
        "print(f\"Max: {df['WORK_LIFE_BALANCE_SCORE'].max():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create histograms for all numerical features\n",
        "# Exclude Timestamp if present in numerical columns\n",
        "plot_cols = [col for col in numerical_cols if col != 'Timestamp']\n",
        "\n",
        "# Calculate grid dimensions\n",
        "n_cols = 4\n",
        "n_rows = int(np.ceil(len(plot_cols) / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(plot_cols):\n",
        "    axes[idx].hist(df[col].dropna(), bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    axes[idx].set_title(f'{col}', fontsize=10, fontweight='bold')\n",
        "    axes[idx].set_xlabel(col, fontsize=9)\n",
        "    axes[idx].set_ylabel('Frequency', fontsize=9)\n",
        "    axes[idx].grid(alpha=0.3)\n",
        "    \n",
        "    # Add mean line\n",
        "    mean_val = df[col].mean()\n",
        "    axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=1.5, alpha=0.7)\n",
        "\n",
        "# Hide extra subplots\n",
        "for idx in range(len(plot_cols), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Distribution of All Numerical Features', fontsize=16, fontweight='bold', y=1.002)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create box plots for key features to visualize spread and outliers\n",
        "key_features = ['FRUITS_VEGGIES', 'DAILY_STRESS', 'PLACES_VISITED', 'CORE_CIRCLE', \n",
        "                'SOCIAL_NETWORK', 'ACHIEVEMENT', 'BMI_RANGE', 'TODO_COMPLETED',\n",
        "                'FLOW', 'DAILY_STEPS', 'SLEEP_HOURS', 'DAILY_SHOUTING',\n",
        "                'TIME_FOR_PASSION', 'WEEKLY_MEDITATION', 'WORK_LIFE_BALANCE_SCORE']\n",
        "\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(key_features) / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(key_features):\n",
        "    bp = axes[idx].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
        "                           boxprops=dict(facecolor='lightcoral', color='darkred', alpha=0.7),\n",
        "                           medianprops=dict(color='blue', linewidth=2),\n",
        "                           whiskerprops=dict(color='darkred'),\n",
        "                           capprops=dict(color='darkred'),\n",
        "                           flierprops=dict(marker='o', markerfacecolor='red', markersize=4, alpha=0.5))\n",
        "    axes[idx].set_title(f'{col}', fontsize=10, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value', fontsize=9)\n",
        "    axes[idx].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Hide extra subplots\n",
        "for idx in range(len(key_features), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Box Plots of Key Features (Outlier Detection)', fontsize=16, fontweight='bold', y=1.002)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Outlier Detection and Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to detect outliers using IQR method\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "# Detect outliers for all numerical columns\n",
        "print(\"=\" * 80)\n",
        "print(\"OUTLIER DETECTION (IQR METHOD)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "outlier_summary = []\n",
        "\n",
        "for col in plot_cols:\n",
        "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
        "    outlier_count = len(outliers)\n",
        "    outlier_percent = (outlier_count / len(df)) * 100\n",
        "    \n",
        "    outlier_summary.append({\n",
        "        'Feature': col,\n",
        "        'Outlier_Count': outlier_count,\n",
        "        'Outlier_Percent': outlier_percent,\n",
        "        'Lower_Bound': lower,\n",
        "        'Upper_Bound': upper\n",
        "    })\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier_Count', ascending=False)\n",
        "print(outlier_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize outliers\n",
        "features_with_outliers = outlier_df[outlier_df['Outlier_Count'] > 0]['Feature'].head(6).tolist()\n",
        "\n",
        "if len(features_with_outliers) > 0:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, col in enumerate(features_with_outliers):\n",
        "        if idx < len(axes):\n",
        "            axes[idx].scatter(range(len(df)), df[col], alpha=0.5, s=10, color='steelblue')\n",
        "            \n",
        "            # Get bounds\n",
        "            outliers, lower, upper = detect_outliers_iqr(df, col)\n",
        "            axes[idx].axhline(y=lower, color='red', linestyle='--', linewidth=2, label=f'Lower: {lower:.2f}')\n",
        "            axes[idx].axhline(y=upper, color='green', linestyle='--', linewidth=2, label=f'Upper: {upper:.2f}')\n",
        "            \n",
        "            axes[idx].set_title(f'{col}\\n({len(outliers)} outliers, {(len(outliers)/len(df)*100):.2f}%)', \n",
        "                               fontsize=10, fontweight='bold')\n",
        "            axes[idx].set_xlabel('Index', fontsize=9)\n",
        "            axes[idx].set_ylabel('Value', fontsize=9)\n",
        "            axes[idx].legend(fontsize=8)\n",
        "            axes[idx].grid(alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(features_with_outliers), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('Outlier Visualization for Top Features', fontsize=14, fontweight='bold', y=1.002)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No significant outliers detected.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier handling strategy\n",
        "print(\"=\" * 80)\n",
        "print(\"OUTLIER HANDLING STRATEGY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a copy for clean data\n",
        "df_clean = df.copy()\n",
        "\n",
        "# For this analysis, we'll use capping method (Winsorization) for extreme outliers\n",
        "# This preserves the data while reducing the impact of extreme values\n",
        "\n",
        "def cap_outliers(data, column, lower_percentile=0.01, upper_percentile=0.99):\n",
        "    \"\"\"Cap outliers at specified percentiles\"\"\"\n",
        "    lower_cap = data[column].quantile(lower_percentile)\n",
        "    upper_cap = data[column].quantile(upper_percentile)\n",
        "    \n",
        "    original_min = data[column].min()\n",
        "    original_max = data[column].max()\n",
        "    \n",
        "    data[column] = data[column].clip(lower=lower_cap, upper=upper_cap)\n",
        "    \n",
        "    return data, lower_cap, upper_cap, original_min, original_max\n",
        "\n",
        "# Apply capping to features with significant outliers (>5% outliers)\n",
        "features_to_cap = outlier_df[outlier_df['Outlier_Percent'] > 5]['Feature'].tolist()\n",
        "\n",
        "print(f\"\\nFeatures with >5% outliers to be capped: {len(features_to_cap)}\")\n",
        "print(features_to_cap)\n",
        "\n",
        "capping_summary = []\n",
        "\n",
        "for col in features_to_cap:\n",
        "    df_clean, lower_cap, upper_cap, orig_min, orig_max = cap_outliers(df_clean, col)\n",
        "    capping_summary.append({\n",
        "        'Feature': col,\n",
        "        'Original_Min': orig_min,\n",
        "        'Original_Max': orig_max,\n",
        "        'Capped_Lower': lower_cap,\n",
        "        'Capped_Upper': upper_cap\n",
        "    })\n",
        "\n",
        "if len(capping_summary) > 0:\n",
        "    print(\"\\nCapping Summary:\")\n",
        "    print(pd.DataFrame(capping_summary).to_string(index=False))\n",
        "else:\n",
        "    print(\"\\nNo features required capping (all features have <5% outliers).\")\n",
        "\n",
        "print(f\"\\nClean dataset shape: {df_clean.shape}\")\n",
        "print(f\"Original dataset shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Summary Statistics by Demographics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure all numerical columns are properly typed\n",
        "# Convert object columns to numeric where appropriate\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA TYPE VERIFICATION AND CONVERSION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nData types before conversion:\")\n",
        "print(df_clean.dtypes)\n",
        "\n",
        "# Convert all numerical feature columns to numeric type\n",
        "for col in numerical_cols:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Data types after conversion:\")\n",
        "print(df_clean.dtypes)\n",
        "\n",
        "# Check if any conversions resulted in NaN\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Missing values after type conversion:\")\n",
        "print(df_clean.isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics by Gender\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY STATISTICS BY GENDER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nGender Distribution:\")\n",
        "print(df_clean['GENDER'].value_counts())\n",
        "print(f\"\\nGender proportions:\")\n",
        "print(df_clean['GENDER'].value_counts(normalize=True) * 100)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"WORK-LIFE BALANCE SCORE BY GENDER\")\n",
        "print(\"=\" * 80)\n",
        "gender_stats = df_clean.groupby('GENDER')['WORK_LIFE_BALANCE_SCORE'].agg([\n",
        "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
        "]).round(2)\n",
        "print(gender_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Work-Life Balance Score by Gender\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Box plot\n",
        "df_clean.boxplot(column='WORK_LIFE_BALANCE_SCORE', by='GENDER', ax=axes[0], patch_artist=True)\n",
        "axes[0].set_title('Work-Life Balance Score by Gender', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Gender', fontsize=11)\n",
        "axes[0].set_ylabel('Work-Life Balance Score', fontsize=11)\n",
        "axes[0].grid(alpha=0.3)\n",
        "plt.sca(axes[0])\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Violin plot\n",
        "import seaborn as sns\n",
        "sns.violinplot(data=df_clean, x='GENDER', y='WORK_LIFE_BALANCE_SCORE', ax=axes[1], palette='Set2')\n",
        "axes[1].set_title('Distribution of Work-Life Balance Score by Gender', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Gender', fontsize=11)\n",
        "axes[1].set_ylabel('Work-Life Balance Score', fontsize=11)\n",
        "axes[1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics by Age Group\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY STATISTICS BY AGE GROUP\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nAge Group Distribution:\")\n",
        "print(df_clean['AGE'].value_counts().sort_index())\n",
        "print(f\"\\nAge Group proportions:\")\n",
        "print(df_clean['AGE'].value_counts(normalize=True).sort_index() * 100)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"WORK-LIFE BALANCE SCORE BY AGE GROUP\")\n",
        "print(\"=\" * 80)\n",
        "age_stats = df_clean.groupby('AGE')['WORK_LIFE_BALANCE_SCORE'].agg([\n",
        "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
        "]).round(2)\n",
        "print(age_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Work-Life Balance Score by Age Group\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Define age order for proper sorting\n",
        "age_order = ['Less than 20', '21 to 35', '36 to 50', '51 or more']\n",
        "\n",
        "# Box plot\n",
        "sns.boxplot(data=df_clean, x='AGE', y='WORK_LIFE_BALANCE_SCORE', ax=axes[0], \n",
        "            order=age_order, palette='viridis')\n",
        "axes[0].set_title('Work-Life Balance Score by Age Group', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Age Group', fontsize=11)\n",
        "axes[0].set_ylabel('Work-Life Balance Score', fontsize=11)\n",
        "axes[0].grid(alpha=0.3, axis='y')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Violin plot\n",
        "sns.violinplot(data=df_clean, x='AGE', y='WORK_LIFE_BALANCE_SCORE', ax=axes[1], \n",
        "               order=age_order, palette='viridis')\n",
        "axes[1].set_title('Distribution of Work-Life Balance Score by Age Group', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Age Group', fontsize=11)\n",
        "axes[1].set_ylabel('Work-Life Balance Score', fontsize=11)\n",
        "axes[1].grid(alpha=0.3, axis='y')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combined Analysis: Gender x Age Group\n",
        "print(\"=\" * 80)\n",
        "print(\"WORK-LIFE BALANCE SCORE BY GENDER AND AGE GROUP\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create pivot table\n",
        "pivot_stats = df_clean.groupby(['AGE', 'GENDER'])['WORK_LIFE_BALANCE_SCORE'].agg([\n",
        "    'count', 'mean', 'std'\n",
        "]).round(2)\n",
        "print(pivot_stats)\n",
        "\n",
        "# Visualize interaction\n",
        "plt.figure(figsize=(12, 6))\n",
        "for gender in df_clean['GENDER'].unique():\n",
        "    gender_data = df_clean[df_clean['GENDER'] == gender]\n",
        "    age_means = gender_data.groupby('AGE')['WORK_LIFE_BALANCE_SCORE'].mean().reindex(age_order)\n",
        "    plt.plot(age_order, age_means, marker='o', linewidth=2, markersize=8, label=gender)\n",
        "\n",
        "plt.title('Work-Life Balance Score by Age and Gender', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Age Group', fontsize=12)\n",
        "plt.ylabel('Mean Work-Life Balance Score', fontsize=12)\n",
        "plt.legend(title='Gender', fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional demographic insights - Key features by gender\n",
        "print(\"=\" * 80)\n",
        "print(\"KEY LIFESTYLE FEATURES BY GENDER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "lifestyle_features = ['FRUITS_VEGGIES', 'DAILY_STRESS', 'SLEEP_HOURS', 'DAILY_STEPS', \n",
        "                      'WEEKLY_MEDITATION', 'TIME_FOR_PASSION', 'FLOW']\n",
        "\n",
        "# Ensure lifestyle features are numeric\n",
        "for col in lifestyle_features:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "gender_lifestyle = df_clean.groupby('GENDER')[lifestyle_features].mean().round(2)\n",
        "print(gender_lifestyle.T)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(lifestyle_features):\n",
        "    sns.barplot(data=df_clean, x='GENDER', y=feature, ax=axes[idx], palette='Set2', ci=None)\n",
        "    axes[idx].set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Gender', fontsize=10)\n",
        "    axes[idx].set_ylabel('Mean Value', fontsize=10)\n",
        "    axes[idx].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Hide last subplot\n",
        "axes[7].axis('off')\n",
        "\n",
        "plt.suptitle('Lifestyle Features by Gender', fontsize=16, fontweight='bold', y=1.002)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key features by age group\n",
        "print(\"=\" * 80)\n",
        "print(\"KEY LIFESTYLE FEATURES BY AGE GROUP\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "age_lifestyle = df_clean.groupby('AGE')[lifestyle_features].mean().round(2)\n",
        "print(age_lifestyle)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(lifestyle_features):\n",
        "    sns.barplot(data=df_clean, x='AGE', y=feature, ax=axes[idx], \n",
        "                order=age_order, palette='viridis', ci=None)\n",
        "    axes[idx].set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Age Group', fontsize=10)\n",
        "    axes[idx].set_ylabel('Mean Value', fontsize=10)\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "    axes[idx].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Hide last subplot\n",
        "axes[7].axis('off')\n",
        "\n",
        "plt.suptitle('Lifestyle Features by Age Group', fontsize=16, fontweight='bold', y=1.002)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Save Clean Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the clean dataset for future use\n",
        "df_clean.to_csv('wellbeing_data_clean.csv', index=False)\n",
        "print(\"Clean dataset saved as 'wellbeing_data_clean.csv'\")\n",
        "print(f\"Shape: {df_clean.shape}\")\n",
        "print(f\"Columns: {df_clean.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2: Feature Engineering & Preprocessing\n",
        "\n",
        "This section creates composite wellness scores and prepares the data for clustering analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Create Composite Wellness Scores\n",
        "\n",
        "We'll create three composite scores representing different dimensions of wellbeing:\n",
        "- **Physical Health Score**: Combines nutrition, BMI, physical activity, and sleep\n",
        "- **Mental Health Score**: Combines stress management, flow state, meditation, and vacation balance\n",
        "- **Social Health Score**: Combines social connections and support networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import additional libraries for preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import os\n",
        "\n",
        "# Create a copy for feature engineering\n",
        "df_features = df_clean.copy()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STARTING FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Starting dataset shape: {df_features.shape}\")\n",
        "print(f\"Columns: {df_features.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to normalize features to 0-10 scale for composite scores\n",
        "def normalize_to_scale(series, target_min=0, target_max=10):\n",
        "    \"\"\"Normalize a series to a target scale (default 0-10)\"\"\"\n",
        "    if series.max() == series.min():\n",
        "        return series\n",
        "    normalized = (series - series.min()) / (series.max() - series.min())\n",
        "    return normalized * (target_max - target_min) + target_min\n",
        "\n",
        "# 1. PHYSICAL HEALTH SCORE\n",
        "print(\"=\" * 80)\n",
        "print(\"CREATING PHYSICAL HEALTH SCORE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Components: FRUITS_VEGGIES, BMI_RANGE, DAILY_STEPS, SLEEP_HOURS\n",
        "physical_components = {\n",
        "    'FRUITS_VEGGIES': df_features['FRUITS_VEGGIES'],\n",
        "    'BMI_RANGE': df_features['BMI_RANGE'],\n",
        "    'DAILY_STEPS': df_features['DAILY_STEPS'],\n",
        "    'SLEEP_HOURS': df_features['SLEEP_HOURS']\n",
        "}\n",
        "\n",
        "# Normalize each component to 0-10 scale\n",
        "physical_normalized = {}\n",
        "for name, component in physical_components.items():\n",
        "    physical_normalized[name] = normalize_to_scale(component)\n",
        "    print(f\"{name}: min={component.min():.2f}, max={component.max():.2f}, mean={component.mean():.2f}\")\n",
        "\n",
        "# Create Physical Health Score (average of normalized components)\n",
        "df_features['PHYSICAL_HEALTH_SCORE'] = (\n",
        "    physical_normalized['FRUITS_VEGGIES'] + \n",
        "    physical_normalized['BMI_RANGE'] + \n",
        "    physical_normalized['DAILY_STEPS'] + \n",
        "    physical_normalized['SLEEP_HOURS']\n",
        ") / 4\n",
        "\n",
        "print(f\"\\nPhysical Health Score created!\")\n",
        "print(f\"Mean: {df_features['PHYSICAL_HEALTH_SCORE'].mean():.2f}\")\n",
        "print(f\"Std: {df_features['PHYSICAL_HEALTH_SCORE'].std():.2f}\")\n",
        "print(f\"Range: [{df_features['PHYSICAL_HEALTH_SCORE'].min():.2f}, {df_features['PHYSICAL_HEALTH_SCORE'].max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. MENTAL HEALTH SCORE\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CREATING MENTAL HEALTH SCORE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Components: DAILY_STRESS (inverted), FLOW, WEEKLY_MEDITATION, LOST_VACATION (inverted)\n",
        "# Note: Lower stress and lower lost vacation are better, so we'll invert them\n",
        "\n",
        "mental_components = {\n",
        "    'DAILY_STRESS': df_features['DAILY_STRESS'],\n",
        "    'FLOW': df_features['FLOW'],\n",
        "    'WEEKLY_MEDITATION': df_features['WEEKLY_MEDITATION'],\n",
        "    'LOST_VACATION': df_features['LOST_VACATION']\n",
        "}\n",
        "\n",
        "# Normalize each component\n",
        "mental_normalized = {}\n",
        "for name, component in mental_components.items():\n",
        "    mental_normalized[name] = normalize_to_scale(component)\n",
        "    print(f\"{name}: min={component.min():.2f}, max={component.max():.2f}, mean={component.mean():.2f}\")\n",
        "\n",
        "# Invert DAILY_STRESS and LOST_VACATION (lower is better)\n",
        "mental_normalized['DAILY_STRESS'] = 10 - mental_normalized['DAILY_STRESS']\n",
        "mental_normalized['LOST_VACATION'] = 10 - mental_normalized['LOST_VACATION']\n",
        "\n",
        "# Create Mental Health Score (average of normalized components)\n",
        "df_features['MENTAL_HEALTH_SCORE'] = (\n",
        "    mental_normalized['DAILY_STRESS'] + \n",
        "    mental_normalized['FLOW'] + \n",
        "    mental_normalized['WEEKLY_MEDITATION'] + \n",
        "    mental_normalized['LOST_VACATION']\n",
        ") / 4\n",
        "\n",
        "print(f\"\\nMental Health Score created!\")\n",
        "print(f\"Mean: {df_features['MENTAL_HEALTH_SCORE'].mean():.2f}\")\n",
        "print(f\"Std: {df_features['MENTAL_HEALTH_SCORE'].std():.2f}\")\n",
        "print(f\"Range: [{df_features['MENTAL_HEALTH_SCORE'].min():.2f}, {df_features['MENTAL_HEALTH_SCORE'].max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. SOCIAL HEALTH SCORE\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CREATING SOCIAL HEALTH SCORE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Components: CORE_CIRCLE, SOCIAL_NETWORK, SUPPORTING_OTHERS\n",
        "social_components = {\n",
        "    'CORE_CIRCLE': df_features['CORE_CIRCLE'],\n",
        "    'SOCIAL_NETWORK': df_features['SOCIAL_NETWORK'],\n",
        "    'SUPPORTING_OTHERS': df_features['SUPPORTING_OTHERS']\n",
        "}\n",
        "\n",
        "# Normalize each component\n",
        "social_normalized = {}\n",
        "for name, component in social_components.items():\n",
        "    social_normalized[name] = normalize_to_scale(component)\n",
        "    print(f\"{name}: min={component.min():.2f}, max={component.max():.2f}, mean={component.mean():.2f}\")\n",
        "\n",
        "# Create Social Health Score (average of normalized components)\n",
        "df_features['SOCIAL_HEALTH_SCORE'] = (\n",
        "    social_normalized['CORE_CIRCLE'] + \n",
        "    social_normalized['SOCIAL_NETWORK'] + \n",
        "    social_normalized['SUPPORTING_OTHERS']\n",
        ") / 3\n",
        "\n",
        "print(f\"\\nSocial Health Score created!\")\n",
        "print(f\"Mean: {df_features['SOCIAL_HEALTH_SCORE'].mean():.2f}\")\n",
        "print(f\"Std: {df_features['SOCIAL_HEALTH_SCORE'].std():.2f}\")\n",
        "print(f\"Range: [{df_features['SOCIAL_HEALTH_SCORE'].min():.2f}, {df_features['SOCIAL_HEALTH_SCORE'].max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the composite scores\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPOSITE SCORES SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "composite_scores = ['PHYSICAL_HEALTH_SCORE', 'MENTAL_HEALTH_SCORE', 'SOCIAL_HEALTH_SCORE']\n",
        "\n",
        "# Check for NaN values in composite scores\n",
        "print(\"\\nChecking for missing values in composite scores:\")\n",
        "for score in composite_scores:\n",
        "    nan_count = df_features[score].isna().sum()\n",
        "    print(f\"  {score}: {nan_count} NaN values\")\n",
        "\n",
        "# Fill NaN values with median for visualization (if any exist)\n",
        "for score in composite_scores:\n",
        "    if df_features[score].isna().sum() > 0:\n",
        "        median_val = df_features[score].median()\n",
        "        df_features[score] = df_features[score].fillna(median_val)\n",
        "        print(f\"  Filled {score} NaN with median: {median_val:.2f}\")\n",
        "\n",
        "summary_stats = df_features[composite_scores].describe().round(2)\n",
        "print(\"\\n\")\n",
        "print(summary_stats)\n",
        "\n",
        "# Create visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Histograms\n",
        "for idx, score in enumerate(composite_scores):\n",
        "    data = df_features[score].dropna()  # Drop NaN for safety\n",
        "    axes[0, idx].hist(data, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0, idx].set_title(f'{score} Distribution', fontsize=11, fontweight='bold')\n",
        "    axes[0, idx].set_xlabel(score, fontsize=10)\n",
        "    axes[0, idx].set_ylabel('Frequency', fontsize=10)\n",
        "    axes[0, idx].axvline(data.mean(), color='red', linestyle='--', \n",
        "                        linewidth=2, label=f'Mean: {data.mean():.2f}')\n",
        "    axes[0, idx].legend()\n",
        "    axes[0, idx].grid(alpha=0.3)\n",
        "\n",
        "# Box plots\n",
        "for idx, score in enumerate(composite_scores):\n",
        "    data = df_features[score].dropna()  # Drop NaN for box plot\n",
        "    axes[1, idx].boxplot(data, vert=True, patch_artist=True,\n",
        "                        boxprops=dict(facecolor='lightgreen', color='darkgreen', alpha=0.7),\n",
        "                        medianprops=dict(color='red', linewidth=2))\n",
        "    axes[1, idx].set_title(f'{score} Box Plot', fontsize=11, fontweight='bold')\n",
        "    axes[1, idx].set_ylabel('Score', fontsize=10)\n",
        "    axes[1, idx].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('Composite Wellness Scores', fontsize=16, fontweight='bold', y=1.002)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Encode Categorical Variables\n",
        "\n",
        "We'll encode AGE and GENDER variables for machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Keep original categorical variables for reference\n",
        "df_features['AGE_ORIGINAL'] = df_features['AGE']\n",
        "df_features['GENDER_ORIGINAL'] = df_features['GENDER']\n",
        "\n",
        "# 1. Label Encoding for AGE (ordinal variable)\n",
        "age_order = ['Less than 20', '21 to 35', '36 to 50', '51 or more']\n",
        "age_mapping = {age: idx for idx, age in enumerate(age_order)}\n",
        "\n",
        "df_features['AGE_ENCODED'] = df_features['AGE'].map(age_mapping)\n",
        "print(\"\\nAge Encoding:\")\n",
        "for age, code in age_mapping.items():\n",
        "    count = (df_features['AGE'] == age).sum()\n",
        "    print(f\"  {age}: {code} (n={count})\")\n",
        "\n",
        "# 2. Binary Encoding for GENDER\n",
        "gender_mapping = {'Male': 1, 'Female': 0}\n",
        "df_features['GENDER_ENCODED'] = df_features['GENDER'].map(gender_mapping)\n",
        "print(\"\\nGender Encoding:\")\n",
        "for gender, code in gender_mapping.items():\n",
        "    count = (df_features['GENDER'] == gender).sum()\n",
        "    print(f\"  {gender}: {code} (n={count})\")\n",
        "\n",
        "# 3. One-Hot Encoding for AGE (for models that benefit from it)\n",
        "age_dummies = pd.get_dummies(df_features['AGE'], prefix='AGE')\n",
        "df_features = pd.concat([df_features, age_dummies], axis=1)\n",
        "\n",
        "print(\"\\nOne-Hot Encoded Age Columns:\")\n",
        "print([col for col in df_features.columns if col.startswith('AGE_')])\n",
        "\n",
        "print(f\"\\nDataset shape after encoding: {df_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Standardize and Normalize Features\n",
        "\n",
        "For clustering algorithms, we need to scale all features to the same range to prevent features with larger scales from dominating the distance calculations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"FEATURE STANDARDIZATION AND NORMALIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select numerical features to scale (exclude original categorical columns and Timestamp)\n",
        "exclude_cols = ['Timestamp', 'AGE', 'GENDER', 'AGE_ORIGINAL', 'GENDER_ORIGINAL']\n",
        "numerical_features = [col for col in df_features.columns if col not in exclude_cols and not col.startswith('AGE_')]\n",
        "\n",
        "print(f\"\\nNumber of numerical features to scale: {len(numerical_features)}\")\n",
        "print(f\"Features: {numerical_features[:10]}...\")  # Show first 10\n",
        "\n",
        "# Create a copy for scaled features\n",
        "df_scaled = df_features.copy()\n",
        "\n",
        "# Method 1: StandardScaler (z-score normalization: mean=0, std=1)\n",
        "# Best for algorithms that assume normally distributed data\n",
        "scaler_standard = StandardScaler()\n",
        "df_scaled[numerical_features] = scaler_standard.fit_transform(df_features[numerical_features])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STANDARDIZATION APPLIED (StandardScaler)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nSample statistics after standardization:\")\n",
        "sample_features = ['WORK_LIFE_BALANCE_SCORE', 'PHYSICAL_HEALTH_SCORE', \n",
        "                   'MENTAL_HEALTH_SCORE', 'SOCIAL_HEALTH_SCORE']\n",
        "for feature in sample_features:\n",
        "    if feature in df_scaled.columns:\n",
        "        print(f\"{feature}:\")\n",
        "        print(f\"  Mean: {df_scaled[feature].mean():.6f}\")\n",
        "        print(f\"  Std: {df_scaled[feature].std():.6f}\")\n",
        "        print(f\"  Min: {df_scaled[feature].min():.2f}\")\n",
        "        print(f\"  Max: {df_scaled[feature].max():.2f}\")\n",
        "\n",
        "# Also create MinMax scaled version (0-1 range)\n",
        "# Better for clustering algorithms like K-Means\n",
        "df_minmax = df_features.copy()\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df_minmax[numerical_features] = scaler_minmax.fit_transform(df_features[numerical_features])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MIN-MAX NORMALIZATION APPLIED (0-1 range)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nSample statistics after min-max scaling:\")\n",
        "for feature in sample_features:\n",
        "    if feature in df_minmax.columns:\n",
        "        print(f\"{feature}:\")\n",
        "        print(f\"  Min: {df_minmax[feature].min():.6f}\")\n",
        "        print(f\"  Max: {df_minmax[feature].max():.6f}\")\n",
        "        print(f\"  Mean: {df_minmax[feature].mean():.4f}\")\n",
        "\n",
        "print(f\"\\nScaled dataset shape: {df_scaled.shape}\")\n",
        "print(f\"MinMax scaled dataset shape: {df_minmax.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the effect of scaling\n",
        "comparison_features = ['FRUITS_VEGGIES', 'DAILY_STRESS', 'WORK_LIFE_BALANCE_SCORE', \n",
        "                       'PHYSICAL_HEALTH_SCORE']\n",
        "\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
        "\n",
        "for idx, feature in enumerate(comparison_features):\n",
        "    # Original\n",
        "    axes[0, idx].hist(df_features[feature], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0, idx].set_title(f'Original: {feature}', fontsize=10, fontweight='bold')\n",
        "    axes[0, idx].set_ylabel('Frequency', fontsize=9)\n",
        "    axes[0, idx].grid(alpha=0.3)\n",
        "    \n",
        "    # Standardized\n",
        "    axes[1, idx].hist(df_scaled[feature], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
        "    axes[1, idx].set_title(f'Standardized: {feature}', fontsize=10, fontweight='bold')\n",
        "    axes[1, idx].set_ylabel('Frequency', fontsize=9)\n",
        "    axes[1, idx].grid(alpha=0.3)\n",
        "    \n",
        "    # Min-Max Scaled\n",
        "    axes[2, idx].hist(df_minmax[feature], bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "    axes[2, idx].set_title(f'Min-Max: {feature}', fontsize=10, fontweight='bold')\n",
        "    axes[2, idx].set_xlabel('Value', fontsize=9)\n",
        "    axes[2, idx].set_ylabel('Frequency', fontsize=9)\n",
        "    axes[2, idx].grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('Comparison of Scaling Methods', fontsize=16, fontweight='bold', y=1.002)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Save Processed Datasets\n",
        "\n",
        "We'll save multiple versions of the processed data for different use cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SAVING PROCESSED DATASETS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "data_dir = 'data'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "    print(f\"Created directory: {data_dir}/\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {data_dir}/\")\n",
        "\n",
        "# Define file paths\n",
        "files_to_save = {\n",
        "    'Original with Features': (df_features, f'{data_dir}/wellbeing_with_features.csv'),\n",
        "    'Standardized (Z-score)': (df_scaled, f'{data_dir}/wellbeing_standardized.csv'),\n",
        "    'MinMax Normalized (0-1)': (df_minmax, f'{data_dir}/processed_wellbeing_data.csv')\n",
        "}\n",
        "\n",
        "# Save all versions\n",
        "print(\"\\nSaving files:\")\n",
        "for description, (dataframe, filepath) in files_to_save.items():\n",
        "    dataframe.to_csv(filepath, index=False)\n",
        "    file_size = os.path.getsize(filepath) / 1024  # Size in KB\n",
        "    print(f\"   {description}: {filepath}\")\n",
        "    print(f\"    Shape: {dataframe.shape}, Size: {file_size:.2f} KB\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nOriginal columns: {len(df.columns)}\")\n",
        "print(f\"New columns added: {len(df_features.columns) - len(df.columns)}\")\n",
        "print(f\"Total columns in processed data: {len(df_features.columns)}\")\n",
        "\n",
        "print(\"\\nNew features created:\")\n",
        "new_features = ['PHYSICAL_HEALTH_SCORE', 'MENTAL_HEALTH_SCORE', 'SOCIAL_HEALTH_SCORE',\n",
        "                'AGE_ENCODED', 'GENDER_ENCODED']\n",
        "for feature in new_features:\n",
        "    if feature in df_features.columns:\n",
        "        print(f\"   {feature}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FILES READY FOR CLUSTERING ANALYSIS!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nRecommended file for clustering: data/processed_wellbeing_data.csv\")\n",
        "print(\"  - All features normalized to 0-1 range\")\n",
        "print(\"  - Composite health scores included\")\n",
        "print(\"  - Categorical variables encoded\")\n",
        "print(\"  - Ready for K-Means, DBSCAN, Hierarchical clustering\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final feature matrix summary\n",
        "print(\"=\" * 80)\n",
        "print(\"FINAL FEATURE MATRIX FOR CLUSTERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show sample of processed data\n",
        "print(\"\\nSample of processed data (first 5 rows, key features):\")\n",
        "key_columns = ['PHYSICAL_HEALTH_SCORE', 'MENTAL_HEALTH_SCORE', 'SOCIAL_HEALTH_SCORE',\n",
        "               'WORK_LIFE_BALANCE_SCORE', 'AGE_ENCODED', 'GENDER_ENCODED']\n",
        "print(df_minmax[key_columns].head())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Data types in processed dataset:\")\n",
        "print(df_minmax.dtypes.value_counts())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" Section 2: Feature Engineering & Preprocessing - COMPLETE\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
